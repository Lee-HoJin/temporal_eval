import pandas as pd
import numpy as np
import json

from metadata import Metadata
from pathlib import Path
import os
from typing import Dict, List, Optional

from sklearn.preprocessing import MinMaxScaler

def load_metadata(real_data_path, dataset_name):
    """ì§€ì •ëœ ë°ì´í„°ì…‹ì˜ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    metadata_path = os.path.join(real_data_path, dataset_name, 'metadata.json')
    with open(metadata_path, 'r') as f:
        return json.load(f)

def scale_features(real_df, synth_df, features):
    real_df = real_df.copy()
    synth_df = synth_df.copy()

    scaler = MinMaxScaler()
    scaler.fit(real_df[features])

    real_df[features] = scaler.transform(real_df[features])
    synth_df[features] = scaler.transform(synth_df[features])
    
    return real_df, synth_df

def get_fk(real_data_path, table_name):
    metadata = Metadata().load_from_json(Path(real_data_path) / f"metadata.json")
    
    fk = metadata.get_primary_key(table_name)
    
    return fk

def get_frequencies(real_data, synthetic_data):
    """
    Get normalized frequency distributions for real and synthetic data.
    
    Args:
        real_data: array-like
        synthetic_data: array-like
    
    Returns:
        f_real: normalized frequencies (probabilities) for real data
        f_syn: normalized frequencies (probabilities) for synthetic data
    """
    # Get all unique categories
    all_categories = sorted(set(real_data) | set(synthetic_data))
    
    # Count frequencies
    real_counts = pd.Series(real_data).value_counts()
    syn_counts = pd.Series(synthetic_data).value_counts()
    
    # Align to all categories (fill missing with 0)
    f_real = np.array([real_counts.get(cat, 0) for cat in all_categories])
    f_syn = np.array([syn_counts.get(cat, 0) for cat in all_categories])
    
    # Normalize to probabilities
    f_real = f_real / (f_real.sum() + 1e-12)
    f_syn = f_syn / (f_syn.sum() + 1e-12)
    
    return f_real, f_syn

def get_datetime_col_info(metadata, table_name):
    """ë©”íƒ€ë°ì´í„°ì—ì„œ ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ì˜ ì´ë¦„ê³¼ í¬ë§·ì„ ì°¾ìŠµë‹ˆë‹¤."""
    for col, info in metadata['tables'][table_name]['columns'].items():
        if info['sdtype'] == 'datetime':
            return col, info.get('datetime_format')
    return None, None

def load_and_preprocess_data(data_path, metadata, parent_table_name, child_table_name):
    """ê°œì„ ëœ ë¶€ëª¨-ìì‹ í…Œì´ë¸” ë³‘í•© (ROBUST VERSION)"""
    
    # 1. ê´€ê³„ ì •ë³´ ì¶”ì¶œ
    relationship = next(
        (r for r in metadata['relationships'] 
         if r['parent_table_name'] == parent_table_name 
         and r['child_table_name'] == child_table_name),
        None
    )
    
    if relationship is None:
        print(f"Error: {parent_table_name}-{child_table_name} ê´€ê³„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None
    
    parent_key = relationship['parent_primary_key']
    child_key = relationship['child_foreign_key']
    
    # 2. ë°ì´í„° ë¡œë“œ
    try:
        parent_df = pd.read_csv(os.path.join(data_path, f"{parent_table_name}.csv"))
        child_df = pd.read_csv(os.path.join(data_path, f"{child_table_name}.csv"))
    except FileNotFoundError as e:
        print(f"Error: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - {e}")
        return None
    
    # 3. í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
    if parent_key not in parent_df.columns:
        print(f"Error: Parent key '{parent_key}' not found in {parent_table_name}")
        return None
    
    if child_key not in child_df.columns:
        print(f"Error: Child key '{child_key}' not found in {child_table_name}")
        return None
    
    # 4. í‚¤ íƒ€ì… í†µì¼ (SIMPLIFIED)
    parent_df, child_df = unify_key_types(
        parent_df, child_df, parent_key, child_key
    )
    
    # 5. ë³‘í•© ì „ ì§„ë‹¨
    diagnosis = diagnose_merge(parent_df, child_df, parent_key, child_key)
    
    if not diagnosis['can_merge']:
        print(f"Error: ë³‘í•© ë¶ˆê°€ëŠ¥ - {diagnosis['reason']}")
        return None
    
    if diagnosis['warnings']:
        for warning in diagnosis['warnings']:
            print(f"Warning: {warning}")
    
    # 6. ë³‘í•© ìˆ˜í–‰
    try:
        merged_df = pd.merge(
            child_df, 
            parent_df, 
            left_on=child_key, 
            right_on=parent_key, 
            how='inner',
            validate='many_to_one'  # ğŸ‘ˆ ê´€ê³„ ê²€ì¦ ì¶”ê°€!
        )
    except pd.errors.MergeError as e:
        print(f"Error: ë³‘í•© ì‹¤íŒ¨ - {e}")
        return None
    
    if merged_df.empty:
        print("Error: ë³‘í•© ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
        return None
    
    print(f"âœ… Merge Completed: {len(merged_df)} rows")
    
    # 7. Datetime ì»¬ëŸ¼ ì²˜ë¦¬
    datetime_col, datetime_format = get_datetime_col_info(metadata, child_table_name)
    
    if datetime_col and datetime_col in merged_df.columns:
        try:
            merged_df[datetime_col] = pd.to_datetime(
                merged_df[datetime_col], 
                # format=datetime_format,
                errors='coerce'
            ).dt.floor('D')
            
            # NaTê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ê²½ê³ 
            nat_ratio = merged_df[datetime_col].isna().sum() / len(merged_df)
            if nat_ratio > 0.1:
                print(f"Warning: {nat_ratio:.1%}ì˜ ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"Warning: datetime ë³€í™˜ ì‹¤íŒ¨ ({e}), datetime_colì„ Noneìœ¼ë¡œ ì„¤ì •")
            datetime_col = None
    
    # 8. ì •ë ¬
    sort_cols = [parent_key]
    if datetime_col and datetime_col in merged_df.columns:
        sort_cols.append(datetime_col)
    
    merged_df = merged_df.sort_values(by=sort_cols).reset_index(drop=True)
    
    return merged_df, parent_key, datetime_col


def unify_key_types(parent_df, child_df, parent_key, child_key):
    """í‚¤ íƒ€ì… í†µì¼ (SIMPLIFIED)"""
    
    # ì›ë³¸ ë°±ì—…
    parent_original = parent_df[parent_key].copy()
    child_original = child_df[child_key].copy()
    
    # ì „ëµ 1: ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
    parent_numeric = pd.to_numeric(parent_df[parent_key], errors='coerce')
    child_numeric = pd.to_numeric(child_df[child_key], errors='coerce')
    
    parent_success = (parent_numeric.notna().sum() / len(parent_numeric)) > 0.9
    child_success = (child_numeric.notna().sum() / len(child_numeric)) > 0.9
    
    if parent_success and child_success:
        parent_df[parent_key] = parent_numeric
        child_df[child_key] = child_numeric
        # print(f"âœ… í‚¤ë¥¼ ìˆ«ìë¡œ ë³€í™˜: {parent_key}, {child_key}")
        return parent_df, child_df
    
    # ì „ëµ 2: ë¬¸ìì—´ë¡œ ë³€í™˜
    parent_df[parent_key] = parent_original.astype(str).str.strip()
    child_df[child_key] = child_original.astype(str).str.strip()
    # print(f"âœ… í‚¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜: {parent_key}, {child_key}")
    
    return parent_df, child_df


def diagnose_merge(parent_df, child_df, parent_key, child_key):
    """ë³‘í•© ê°€ëŠ¥ ì—¬ë¶€ ì§„ë‹¨ (NO HEURISTIC FALLBACK!)"""
    
    result = {
        'can_merge': False,
        'reason': '',
        'warnings': []
    }
    
    # NaN ì²´í¬
    parent_valid = parent_df[parent_key].dropna()
    child_valid = child_df[child_key].dropna()
    
    parent_nan_ratio = (len(parent_df) - len(parent_valid)) / len(parent_df)
    child_nan_ratio = (len(child_df) - len(child_valid)) / len(child_df)
    
    if parent_nan_ratio > 0.1:
        result['warnings'].append(
            f"Parent keyì— {parent_nan_ratio:.1%} NaN (ë³‘í•© ì‹œ ì œì™¸ë¨)"
        )
    
    if child_nan_ratio > 0.1:
        result['warnings'].append(
            f"Child keyì— {child_nan_ratio:.1%} NaN (ë³‘í•© ì‹œ ì œì™¸ë¨)"
        )
    
    if len(parent_valid) == 0 or len(child_valid) == 0:
        result['reason'] = "ìœ íš¨í•œ í‚¤ê°€ ì—†ìŒ"
        return result
    
    # ê³µí†µ í‚¤ í™•ì¸
    parent_keys = set(parent_valid)
    child_keys = set(child_valid)
    common_keys = parent_keys.intersection(child_keys)
    
    match_ratio = len(common_keys) / len(child_keys)
    
    # print(f"ğŸ“Š Parent ê³ ìœ  í‚¤: {len(parent_keys)}")
    # print(f"ğŸ“Š Child ê³ ìœ  í‚¤: {len(child_keys)}")
    # print(f"ğŸ“Š ê³µí†µ í‚¤: {len(common_keys)} ({match_ratio:.1%})")
    
    # âœ… ê³µí†µ í‚¤ê°€ ì—†ìœ¼ë©´ ë³‘í•© ë¶ˆê°€ (NO FALLBACK!)
    if len(common_keys) == 0:
        result['reason'] = "ê³µí†µ í‚¤ê°€ ì—†ìŒ - FK ê´€ê³„ê°€ ì†ìƒë¨"
        print(f"Parent í‚¤ ìƒ˜í”Œ: {list(parent_keys)[:5]}")
        print(f"Child í‚¤ ìƒ˜í”Œ: {list(child_keys)[:5]}")
        return result
    
    # ë§¤ì¹­ë¥ ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ê²½ê³ 
    if match_ratio < 0.5:
        result['warnings'].append(
            f"Childì˜ {match_ratio:.1%}ë§Œ Parentì™€ ë§¤ì¹­ë¨ "
            f"({len(child_keys) - len(common_keys)}ê°œ ê³ ì•„ ë ˆì½”ë“œ ì œì™¸ë¨)"
        )
    
    # Cardinality ì²´í¬ (Cartesian product ë°©ì§€)
    sample_common = list(common_keys)[:10]
    total_expected = 0
    
    for key in sample_common:
        parent_count = (parent_valid == key).sum()
        child_count = (child_valid == key).sum()
        total_expected += parent_count * child_count
    
    avg_per_key = total_expected / len(sample_common)
    estimated_total = avg_per_key * len(common_keys)
    
    if estimated_total > 10_000_000:  # 1000ë§Œ í–‰ ì´ìƒ
        result['reason'] = f"ì˜ˆìƒ ë³‘í•© í¬ê¸°ê°€ ë„ˆë¬´ í¼ ({estimated_total:.0f} rows)"
        return result
    
    if estimated_total > 1_000_000:  # 100ë§Œ í–‰ ì´ìƒ
        result['warnings'].append(
            f"ì˜ˆìƒ ë³‘í•© í¬ê¸°: {estimated_total:.0f} rows (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ)"
        )
    
    result['can_merge'] = True
    return result


def debug_temporal_overlap(real_df, synth_df, time_col='Date', target_col='IsHoliday'):
    print(f"ğŸ” ë””ë²„ê¹…: {target_col} ì»¬ëŸ¼ ë° ì‹œê°„ ë²”ìœ„ í™•ì¸")
    print("-" * 50)
    
    # 1. ë‚ ì§œ í˜•ì‹ ë³€í™˜ ë° ë²”ìœ„ í™•ì¸
    real_df[time_col] = pd.to_datetime(real_df[time_col])
    synth_df[time_col] = pd.to_datetime(synth_df[time_col])
    
    r_min, r_max = real_df[time_col].min(), real_df[time_col].max()
    s_min, s_max = synth_df[time_col].min(), synth_df[time_col].max()
    
    print(f"ğŸ“… Real Date Range : {r_min} ~ {r_max}")
    print(f"ğŸ“… Synth Date Range: {s_min} ~ {s_max}")
    
    # 2. ê²¹ì¹˜ëŠ” ê¸°ê°„ í™•ì¸
    overlap_start = max(r_min, s_min)
    overlap_end = min(r_max, s_max)
    
    if overlap_start > overlap_end:
        print("âŒ [CRITICAL] ê²¹ì¹˜ëŠ” ì‹œê°„ êµ¬ê°„ì´ ì „í˜€ ì—†ìŠµë‹ˆë‹¤! (JSD = 0ì˜ ì›ì¸)")
        return
    else:
        print(f"âœ… Overlap Period  : {overlap_start} ~ {overlap_end}")

    # 3. ë°ì´í„° íƒ€ì… í™•ì¸
    print(f"\nğŸ·ï¸ Data Types:")
    print(f"   Real [{target_col}]: {real_df[target_col].dtype} (ì˜ˆ: {real_df[target_col].iloc[0]})")
    print(f"   Synth [{target_col}]: {synth_df[target_col].dtype} (ì˜ˆ: {synth_df[target_col].iloc[0]})")
    
    if real_df[target_col].dtype != synth_df[target_col].dtype:
        print("âš ï¸ [WARNING] ë°ì´í„° íƒ€ì…ì´ ë‹¤ë¦…ë‹ˆë‹¤! (Bool vs Float ë“±)")
        print("   -> ë¹„êµ ì „ í†µì¼ í•„ìš” (ì˜ˆ: .astype(str) or .astype(int))")

    # 4. ì‹¤ì œ êµ¬ê°„ë³„ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ ìƒ˜í”Œë§ (ì²« ì›”/ì£¼)
    # ì›”ë‹¨ìœ„ binning ì˜ˆì‹œ
    sample_bin = real_df[time_col].dt.to_period('M').astype(str).unique()[0]
    
    r_count = real_df[real_df[time_col].dt.to_period('M').astype(str) == sample_bin].shape[0]
    s_count = synth_df[synth_df[time_col].dt.to_period('M').astype(str) == sample_bin].shape[0]
    
    print(f"\nğŸ“¦ Sample Bin ({sample_bin}) Counts:")
    print(f"   Real: {r_count} rows")
    print(f"   Synth: {s_count} rows")
    
    if r_count > 0 and s_count == 0:
        print("âŒ [CHECK] Real ë°ì´í„°ëŠ” ìˆëŠ”ë° Synth ë°ì´í„°ê°€ í•´ë‹¹ êµ¬ê°„ì— ì—†ìŠµë‹ˆë‹¤.")